{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ea5c26-d6c2-4e71-a58c-217d10d63b4f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Notebook does following \n",
    "\n",
    "- ðŸ—ï¸ Step 1: Create case-insensitive Data Warehouse.\n",
    "- ðŸ”¹ Step 2: Connect to Synapse Serverless Database and get table list and schema information.\n",
    "- ðŸ” Step 3: Get parent and child tables information from local file.\n",
    "- ðŸ› ï¸ Step 4: Connect to Fabric Link lakehouse get table metadata and generate view ddl statements.\n",
    "- ðŸƒâ€â™‚ï¸ Step 5: Connect to Fabric case in-senstive data warehouse and create views.\n",
    "- ðŸš€ Step 6: Connect to Synaspe serverless virtual datawarehouse and collect views and dependencies.\n",
    "- ðŸš€ Step 7: Connect to Fabric datawarehouse and deploy create views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50733e90-a4fc-4424-896b-16399342f769",
   "metadata": {
    "cellStatus": "{\"JJ Yadav\":{\"session_start_time\":\"2025-06-07T13:05:35.2155677Z\",\"execution_start_time\":\"2025-06-07T13:05:39.2399932Z\",\"execution_finish_time\":\"2025-06-07T13:05:40.0626782Z\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-08T12:05:27.9366666Z",
       "execution_start_time": "2025-06-08T12:05:27.6196121Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "54c18d16-8930-4c61-af04-afe29848aa12",
       "queued_time": "2025-06-08T12:05:27.61839Z",
       "session_id": "65f6907a-b648-449a-a793-62399681b712",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 65f6907a-b648-449a-a793-62399681b712, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Configuration Constants ---\n",
    "WORKSPACE_ID        = \"cca1475b-c4fe-417f-844a-f3e8a061e55d\"\n",
    "FABRIC_LH_DATABASE  = \"dataverse_jjunoenvfre\"\n",
    "FABRIC_WH_DATABASE  = \"Sales_DW\"\n",
    "\n",
    "#Optional parameters to connect to your Synapse Serveress to get table and schema info\n",
    "DRIVER              = \"{ODBC Driver 18 for SQL Server}\"\n",
    "SYNAPSE_SERVER      = None # update to example value \"d365analyticsfabricsynapse-ondemand.sql.azuresynapse.net\"  \n",
    "SYNAPSE_EDL_DATABASE= None #update to example value \"analytics.sandbox.operations.dynamics.com\" \n",
    "SYNAPSE_EDL_SCHEMA  = None  #\"dbo\" \n",
    "SYNAPSE_EDL_CONST_COLUMN_NAME = None #=\"_SysRowId\" \n",
    "\n",
    "\n",
    "#Optional parameters to connect to your Synapse Serveress Datawarehouse to get views and dependencies\n",
    "SYNAPSE_DW_DATABASE = None # example value \"Dynamics365_DW\"\n",
    "SYNAPSE_DW_SCHEMA   = None # example value \"dbo\"\n",
    "SYNAPSE_DW_VIEWS    = None # example value \"CustomerDim,SalesFact,SupplierDim\"\n",
    "FABRIC_WH_SCHEMA    = None # example value \"edw\"\n",
    "\n",
    "#Fixed parameter - No need to change\n",
    "GITHUB_RAW_BASE_URL = \"https://raw.githubusercontent.com/microsoft/Dynamics-365-FastTrack-Implementation-Assets/refs/heads/master/Analytics/DataverseLink/FabricLink_SQLAnalyticsEndpoint/DVFabricLinkUtil/\"\n",
    "\n",
    "DERIVED_TABLE_MAP_PATH = \"./builtin/resources/derived_table_map.json\"\n",
    "LH_DDL_TEMPLATE_PATH = \"./builtin/resources/get_lh_ddl_as_view.sql\"\n",
    "VIEW_DEPENDENCY_TEMPLATE_PATH = \"./builtin/resources/get_view_dependency.sql\"\n",
    "REQUIRED_FILES = [\n",
    "    (\"derived_table_map.json\", DERIVED_TABLE_MAP_PATH),\n",
    "    (\"get_lh_ddl_as_view.sql\", LH_DDL_TEMPLATE_PATH),\n",
    "    (\"get_view_dependency.sql\", VIEW_DEPENDENCY_TEMPLATE_PATH)]\n",
    "\n",
    "WAREHOUSE_VIEW_DDL_PARAMETERS = {\n",
    "            \"source_schema\": \"dbo\",\n",
    "            \"target_schema\": \"dbo\",\n",
    "            \"only_fno_tables\": 1,\n",
    "            \"tables_to_include\":\"*\",\n",
    "            \"tables_to_exclude\": \"*\",\n",
    "            \"filter_deleted_rows\": 1,\n",
    "            \"join_derived_tables\": 1,\n",
    "            \"change_collation\": 1,\n",
    "            \"translate_enums\": 1,\n",
    "            \"schema_map\": '[]',\n",
    "            \"derived_table_map\": '[]'\n",
    "        }\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2d3a9a-f5c8-4078-a36f-b540e44e6bfd",
   "metadata": {
    "cellStatus": "{\"JJ Yadav\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-07T13:05:40.0644055Z\",\"execution_finish_time\":\"2025-06-07T13:06:28.2937375Z\",\"normalized_state\":\"finished\"}}",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-08T12:05:37.4715525Z",
       "execution_start_time": "2025-06-08T12:05:32.5356484Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a1895d02-a2eb-4c23-84a4-f0a95345f6d9",
       "queued_time": "2025-06-08T12:05:32.5337622Z",
       "session_id": "65f6907a-b648-449a-a793-62399681b712",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 65f6907a-b648-449a-a793-62399681b712, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-08 12:05:34] INFO - ðŸ—ï¸ Step 1: Download template files if does not exists.\n",
      "[2025-06-08 12:05:34] DEBUG - Command to send: r\n",
      "m\n",
      "notebookutils.fs\n",
      "exists\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:34] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:34] DEBUG - Command to send: c\n",
      "z:notebookutils.fs\n",
      "exists\n",
      "s./builtin/resources/derived_table_map.json\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !xro6437\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.sql.catalyst.parser.ParseException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.sql.AnalysisException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.sql.streaming.StreamingQueryException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.sql.execution.QueryExecutionException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sjava.lang.NumberFormatException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sjava.lang.IllegalArgumentException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sjava.lang.ArithmeticException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sjava.lang.UnsupportedOperationException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sjava.lang.ArrayIndexOutOfBoundsException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sjava.time.DateTimeException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.SparkRuntimeException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycpy4j.reflection.TypeUtil\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.SparkUpgradeException\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ybfalse\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "o6437\n",
      "getCause\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yn\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yp\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ycorg.apache.spark.util.Utils\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "exceptionString\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ym\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "exceptionString\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ysOperation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/cca1475b-c4fe-417f-844a-f3e8a061e55d/user/trusted-service-user/builtin/resources/derived_table_map.json?upn=false&action=getStatus&timeout=90\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:779)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1067)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$2(MSFsUtilsImpl.scala:638)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.fsTSG(MSFsUtilsImpl.scala:223)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$1(MSFsUtilsImpl.scala:635)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.exists(MSFsUtilsImpl.scala:635)\\n\tat mssparkutils.IFs.exists(fs.scala:41)\\n\tat mssparkutils.IFs.exists$(fs.scala:41)\\n\tat notebookutils.fs$.exists(utils.scala:12)\\n\tat notebookutils.fs.exists(utils.scala)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\tat py4j.Gateway.invoke(Gateway.java:282)\\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: c\n",
      "o6437\n",
      "toString\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !ysOperation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/cca1475b-c4fe-417f-844a-f3e8a061e55d/user/trusted-service-user/builtin/resources/derived_table_map.json?upn=false&action=getStatus&timeout=90\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: m\n",
      "d\n",
      "o3736\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yv\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: m\n",
      "d\n",
      "o3743\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yv\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: m\n",
      "d\n",
      "o4053\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yv\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: m\n",
      "d\n",
      "o5091\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yv\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: m\n",
      "d\n",
      "o5084\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yv\n",
      "[2025-06-08 12:05:35] DEBUG - Command to send: m\n",
      "d\n",
      "o5381\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:35] DEBUG - Answer received: !yv\n",
      "[2025-06-08 12:05:36] DEBUG - Command to send: p\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:36] DEBUG - Answer received: !ysOperation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/cca1475b-c4fe-417f-844a-f3e8a061e55d/user/trusted-service-user/builtin/resources/derived_table_map.json?upn=false&action=getStatus&timeout=90\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:779)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1067)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$2(MSFsUtilsImpl.scala:638)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.fsTSG(MSFsUtilsImpl.scala:223)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$1(MSFsUtilsImpl.scala:635)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.exists(MSFsUtilsImpl.scala:635)\\n\tat mssparkutils.IFs.exists(fs.scala:41)\\n\tat mssparkutils.IFs.exists$(fs.scala:41)\\n\tat notebookutils.fs$.exists(utils.scala:12)\\n\tat notebookutils.fs.exists(utils.scala)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\tat py4j.Gateway.invoke(Gateway.java:282)\\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\n",
      "[2025-06-08 12:05:36] DEBUG - Command to send: p\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:36] DEBUG - Answer received: !ysOperation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/cca1475b-c4fe-417f-844a-f3e8a061e55d/user/trusted-service-user/builtin/resources/derived_table_map.json?upn=false&action=getStatus&timeout=90\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:779)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1067)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$2(MSFsUtilsImpl.scala:638)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.fsTSG(MSFsUtilsImpl.scala:223)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$1(MSFsUtilsImpl.scala:635)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.exists(MSFsUtilsImpl.scala:635)\\n\tat mssparkutils.IFs.exists(fs.scala:41)\\n\tat mssparkutils.IFs.exists$(fs.scala:41)\\n\tat notebookutils.fs$.exists(utils.scala:12)\\n\tat notebookutils.fs.exists(utils.scala)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\tat py4j.Gateway.invoke(Gateway.java:282)\\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:notebookutils.fs.exists.\n: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/cca1475b-c4fe-417f-844a-f3e8a061e55d/user/trusted-service-user/builtin/resources/derived_table_map.json?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:779)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1067)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$2(MSFsUtilsImpl.scala:638)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.fsTSG(MSFsUtilsImpl.scala:223)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$1(MSFsUtilsImpl.scala:635)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.exists(MSFsUtilsImpl.scala:635)\n\tat mssparkutils.IFs.exists(fs.scala:41)\n\tat mssparkutils.IFs.exists$(fs.scala:41)\n\tat notebookutils.fs$.exists(utils.scala:12)\n\tat notebookutils.fs.exists(utils.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 279\u001b[0m\n\u001b[1;32m    275\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ Warehouse setup failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[26], line 226\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, local_path \u001b[38;5;129;01min\u001b[39;00m REQUIRED_FILES:\n\u001b[1;32m    225\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGITHUB_RAW_BASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 226\u001b[0m     download_file_if_not_exists(file_url, local_path)\n\u001b[1;32m    228\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ—ï¸ Step 2/7: Ensure case-insensitive warehouse exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_case_insensitive_warehouse(WORKSPACE_ID, FABRIC_WH_DATABASE):\n",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m, in \u001b[0;36mdownload_file_if_not_exists\u001b[0;34m(url, local_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_file_if_not_exists\u001b[39m(url, local_path):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download a file from GitHub if it doesn't exist locally.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m notebookutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mexists(local_path):\n\u001b[1;32m     21\u001b[0m         notebookutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmkdirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(local_path))\n\u001b[1;32m     22\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ¬‡ï¸ Downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/notebookutils/fs.py:43\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexists\u001b[39m(file):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(file)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/notebookutils/mssparkutils/handlers/fsHandler.py:137\u001b[0m, in \u001b[0;36mSynapseFSHandler.exists\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexists\u001b[39m(\u001b[38;5;28mself\u001b[39m, file):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_types([(file, string_types)])\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfsutils\u001b[38;5;241m.\u001b[39mexists(file)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:notebookutils.fs.exists.\n: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/cca1475b-c4fe-417f-844a-f3e8a061e55d/user/trusted-service-user/builtin/resources/derived_table_map.json?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:779)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1067)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$2(MSFsUtilsImpl.scala:638)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.fsTSG(MSFsUtilsImpl.scala:223)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$1(MSFsUtilsImpl.scala:635)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.exists(MSFsUtilsImpl.scala:635)\n\tat mssparkutils.IFs.exists(fs.scala:41)\n\tat mssparkutils.IFs.exists$(fs.scala:41)\n\tat notebookutils.fs$.exists(utils.scala:12)\n\tat notebookutils.fs.exists(utils.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-08 12:05:36] DEBUG - Command to send: p\n",
      "ro6437\n",
      "e\n",
      "\n",
      "[2025-06-08 12:05:36] DEBUG - Answer received: !ysOperation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/cca1475b-c4fe-417f-844a-f3e8a061e55d/user/trusted-service-user/builtin/resources/derived_table_map.json?upn=false&action=getStatus&timeout=90\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:779)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1067)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$2(MSFsUtilsImpl.scala:638)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.fsTSG(MSFsUtilsImpl.scala:223)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.$anonfun$exists$1(MSFsUtilsImpl.scala:635)\\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\\n\tat com.microsoft.spark.notebook.msutils.impl.MSFsUtilsImpl.exists(MSFsUtilsImpl.scala:635)\\n\tat mssparkutils.IFs.exists(fs.scala:41)\\n\tat mssparkutils.IFs.exists$(fs.scala:41)\\n\tat notebookutils.fs$.exists(utils.scala:12)\\n\tat notebookutils.fs.exists(utils.scala)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\tat py4j.Gateway.invoke(Gateway.java:282)\\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import struct\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from IPython.display import display, JSON\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from requests.exceptions import HTTPError\n",
    "import sys\n",
    "import os\n",
    "\n",
    "synapse_edl_engine = None\n",
    "synapse_dw_engine = None\n",
    "\n",
    "def download_file_if_not_exists(url, local_path):\n",
    "    \"\"\"Download a file from GitHub if it doesn't exist locally.\"\"\"\n",
    "    if not notebookutils.fs.exists(local_path):\n",
    "        notebookutils.fs.mkdirs(os.path.dirname(local_path))\n",
    "        logger.info(f\"â¬‡ï¸ Downloading {local_path} ...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Fail if not 200 OK\n",
    "        notebookutils.fs.put(local_path, response.content.decode('utf-8'))  # <-- decode bytes to string\n",
    "    else:\n",
    "        logger.info(f\"ðŸ“„ File already exists locally: {local_path}\")\n",
    "\n",
    "def is_not_none_and_empty(s):\n",
    "    return s is not None and s != ''\n",
    "\n",
    "\n",
    "# --- Database Connections ---\n",
    "def create_synapse_engine(server, database):\n",
    "    connection_string = f\"DRIVER={DRIVER};SERVER={server};DATABASE={database};Encrypt=yes;TrustServerCertificate=no;\"\n",
    "    odbc_conn_str = f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(connection_string)}\"\n",
    "    engine = create_engine(odbc_conn_str)\n",
    "    return engine\n",
    "\n",
    "synapse_edl_engine = create_synapse_engine(SYNAPSE_SERVER, SYNAPSE_EDL_DATABASE)\n",
    "synapse_dw_engine = create_synapse_engine(SYNAPSE_SERVER, SYNAPSE_DW_DATABASE)\n",
    "\n",
    "@event.listens_for(synapse_edl_engine, \"do_connect\")\n",
    "@event.listens_for(synapse_dw_engine, \"do_connect\")\n",
    "def inject_access_token(dialect, conn_rec, cargs, cparams):\n",
    "    token = notebookutils.credentials.getToken(\"https://database.windows.net/\")\n",
    "    token_bytes = token.encode(\"utf-16-le\")\n",
    "    token_struct = struct.pack(f\"<I{len(token_bytes)}s\", len(token_bytes), token_bytes)\n",
    "    cparams[\"attrs_before\"] = {1256: token_struct}\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def load_builtin_file(path):\n",
    "    \"\"\"Load a built-in SQL or JSON file.\"\"\"\n",
    "    return notebookutils.fs.head(path)\n",
    "\n",
    "def connect_to_fabric_artifact(artifact_name, workspace_id):\n",
    "    return notebookutils.data.connect_to_artifact(artifact_name, workspace_id)\n",
    "\n",
    "# --- Warehouse Management ---\n",
    "def warehouse_exists(workspace_id, warehouse_name):\n",
    "    try:\n",
    "        connect_to_fabric_artifact(warehouse_name, workspace_id)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if \"ArtifactNotFoundException\" in str(type(e)):\n",
    "            logger.info(f\"ðŸ” Warehouse not found:{warehouse_name}\")\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def create_case_insensitive_warehouse(workspace_id, warehouse_name, retries=5, delay=10):\n",
    "    if warehouse_exists(workspace_id, warehouse_name):\n",
    "        logger.info(f\"âœ… Warehouse '{warehouse_name}' already exists.\")\n",
    "        return True\n",
    "\n",
    "    logger.info(f\"ðŸ—ï¸ Creating new warehouse:{warehouse_name}\")\n",
    "    token = notebookutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"type\": \"Warehouse\",\n",
    "        \"displayName\": warehouse_name,\n",
    "        \"description\": \"New warehouse with case-insensitive collation\",\n",
    "        \"creationPayload\": {\"defaultCollation\": \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"}\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\",\n",
    "        headers=headers, data=json.dumps(payload)\n",
    "    )\n",
    "    logger.info(f\"ðŸ“¨ Warehouse creation response: {response.text}\", )\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        time.sleep(delay)\n",
    "        if warehouse_exists(workspace_id, warehouse_name):\n",
    "            logger.info(\"âœ… Warehouse is ready.\")\n",
    "            return True\n",
    "        logger.info(f\"ðŸ”„ Retry {attempt + 1}/{retries}: Warehouse not yet available.\")\n",
    "\n",
    "    logger.error(\"âŒ Failed to create warehouse after retries.\")\n",
    "    return False\n",
    "\n",
    "# --- Schema Retrieval ---\n",
    "def fetch_tables_and_schema_map(engine):\n",
    "    \"\"\"Fetch table list and schema mapping.\"\"\"\n",
    "    table_list_query = f\"\"\"\n",
    "        SELECT STRING_AGG(CONVERT(NVARCHAR(MAX), TABLE_NAME), ',') AS tablelist\n",
    "        FROM (\n",
    "            SELECT DISTINCT LOWER(TABLE_NAME) as TABLE_NAME\n",
    "            FROM INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' \n",
    "              AND TABLE_NAME IN (\n",
    "                  SELECT DISTINCT TABLE_NAME \n",
    "                  FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                  WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' \n",
    "                    AND COLUMN_NAME = '{SYNAPSE_EDL_CONST_COLUMN_NAME}'\n",
    "              )\n",
    "        ) AS tbl\n",
    "    \"\"\"\n",
    "    schema_map_query = f\"\"\"\n",
    "        SELECT TABLE_NAME as tablename, COLUMN_NAME as columnname,\n",
    "            DATA_TYPE + \n",
    "            CASE \n",
    "                WHEN CHARACTER_MAXIMUM_LENGTH IS NOT NULL THEN '(' + CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)) + ')'\n",
    "                WHEN CHARACTER_MAXIMUM_LENGTH = -1 THEN '(max)'\n",
    "                WHEN DATA_TYPE = 'decimal' THEN '(' + CAST(NUMERIC_PRECISION AS VARCHAR(10)) + ',' + CAST(NUMERIC_SCALE AS VARCHAR(10)) + ')'\n",
    "                ELSE '' \n",
    "            END AS datatype\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}'\n",
    "          AND TABLE_NAME IN (\n",
    "              SELECT DISTINCT TABLE_NAME \n",
    "              FROM INFORMATION_SCHEMA.COLUMNS\n",
    "              WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' AND COLUMN_NAME = '{SYNAPSE_EDL_CONST_COLUMN_NAME}'\n",
    "          )\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        table_list = conn.execute(text(table_list_query)).scalar()\n",
    "    schema_map_df = pd.read_sql(schema_map_query, engine)\n",
    "    schema_map_json = schema_map_df.to_json(orient=\"records\")\n",
    "\n",
    "    return table_list, schema_map_json\n",
    "\n",
    "# --- DDL and View Creation ---\n",
    "def generate_lh_view_ddl(lakehouse_name, params):\n",
    "    \"\"\"Generate view DDL based on Lakehouse tables.\"\"\"\n",
    "    conn = connect_to_fabric_artifact(lakehouse_name, WORKSPACE_ID)\n",
    "\n",
    "    variables_script = f\"\"\"\n",
    "        DECLARE \n",
    "            @source_database_name VARCHAR(200) = '{lakehouse_name}',\n",
    "            @source_table_schema NVARCHAR(10) = '{params[\"source_schema\"]}',\n",
    "            @target_table_schema NVARCHAR(10) = '{params[\"target_schema\"]}',\n",
    "            @TablesToInclude_FnOOnly INT = {params[\"only_fno_tables\"]},\n",
    "            @TablesToIncluce NVARCHAR(MAX) = '{params[\"tables_to_include\"]}',\n",
    "            @TablesToExcluce NVARCHAR(MAX) = '{params[\"tables_to_exclude\"]}',\n",
    "            @filter_deleted_rows INT = {params[\"filter_deleted_rows\"]},\n",
    "            @join_derived_tables INT = {params[\"join_derived_tables\"]},\n",
    "            @change_column_collation INT = {params[\"change_collation\"]},\n",
    "            @translate_enums INT = {params[\"translate_enums\"]},\n",
    "            @schema_map VARCHAR(MAX) = {params[\"schema_map\"]},\n",
    "            @tableinheritance NVARCHAR(MAX) = LOWER('{params[\"derived_table_map\"]}');\n",
    "    \"\"\"\n",
    "    sql_template = load_builtin_file(LH_DDL_TEMPLATE_PATH)\n",
    "    \n",
    "    sqlquery = f\"{variables_script} {sql_template}\"\n",
    "\n",
    "    logger.debug(f\"Debug: generate_lh_view_ddl query {sqlquery}\")\n",
    "    df = conn.query(f\"{sqlquery}\")\n",
    "    ddl_statement = df.iloc[0, 0]\n",
    "    logger.debug(f\"Debug: generate_lh_view_ddl statement {sqlquery}\")\n",
    "\n",
    "    return ddl_statement\n",
    "\n",
    "def execute_ddl_on_warehouse(warehouse_name, ddl_query):\n",
    "    \"\"\"Execute DDL in warehouse.\"\"\"\n",
    "    conn = connect_to_fabric_artifact(warehouse_name, WORKSPACE_ID)\n",
    "    conn.query(ddl_query)\n",
    "\n",
    "# --- View Dependency Management ---\n",
    "def fetch_view_dependencies(engine, root_entities, old_db, new_db, old_schema, new_schema):\n",
    "    sql_template = load_builtin_file(VIEW_DEPENDENCY_TEMPLATE_PATH)\n",
    "    sql_script = f\"\"\"\n",
    "        SET NOCOUNT ON;\n",
    "        DROP TABLE IF EXISTS #myEntitiestree;\n",
    "        DECLARE @entities NVARCHAR(MAX) = '{root_entities}';\n",
    "        DECLARE @old_schema VARCHAR(10) = '{old_schema}';\n",
    "        DECLARE @new_schema VARCHAR(10) = '{new_schema}';\n",
    "        {sql_template}\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(sql_script))\n",
    "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    df[\"definition\"] = df[\"definition\"].str.replace(old_db, new_db, case=False, regex=False)\n",
    "    return df.to_json(orient=\"records\")\n",
    "\n",
    "def deploy_views(warehouse_name, schema, views_json):\n",
    "    execute_ddl_on_warehouse(warehouse_name, f\"IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = '{schema}') BEGIN EXEC('CREATE SCHEMA {schema}'); END;\")\n",
    "    views = json.loads(views_json)\n",
    "    for view in sorted(views, key=lambda x: x[\"depth\"], reverse=True):\n",
    "        if view.get(\"definition\"):\n",
    "            try:\n",
    "                entityname = view[\"entityName\"]\n",
    "                execute_ddl_on_warehouse(warehouse_name, view[\"definition\"])\n",
    "                logger.info(f\"âœ… Deployed view: {entityname}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed to deploy view {entityname}: {e}\")\n",
    "\n",
    "for handler in logging.root.handlers[:]:  # Clear existing handlers\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]  # Explicitly use stdout\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()  \n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \n",
    "    logger.info(\"ðŸ—ï¸ Step 1: Download template files if does not exists.\")\n",
    "    # --- Download if not exists ---\n",
    "    for filename, local_path in REQUIRED_FILES:\n",
    "        file_url = f\"{GITHUB_RAW_BASE_URL}/{filename}\"\n",
    "        download_file_if_not_exists(file_url, local_path)\n",
    "        \n",
    "    logger.info(\"ðŸ—ï¸ Step 2/7: Ensure case-insensitive warehouse exists.\")\n",
    "    \n",
    "    if create_case_insensitive_warehouse(WORKSPACE_ID, FABRIC_WH_DATABASE):\n",
    "\n",
    "        logger.info(\"ðŸ”¹ Step 3/7: Fetch tables and schema map from Synapse.\")\n",
    "        tables_to_include = '*'\n",
    "        schema_map = '[]'\n",
    "        if is_not_none_and_empty(SYNAPSE_SERVER) and is_not_none_and_empty(SYNAPSE_EDL_DATABASE) :\n",
    "          \n",
    "            tables_to_include, schema_map = fetch_tables_and_schema_map(synapse_edl_engine)\n",
    "        else:\n",
    "            logger.info(\"ðŸ”¹ Step 3/7: Skipped\")\n",
    "\n",
    "        logger.info(\"ðŸ” Step 4/7: Load derived table map.\")\n",
    "        derived_table_map = load_builtin_file(DERIVED_TABLE_MAP_PATH)\n",
    "\n",
    "        logger.info(\"ðŸ› ï¸ Step 5/7: Generate view DDL.\")\n",
    "        \n",
    "        # Create a copy of the parameters and update the dynamic fields\n",
    "        params = WAREHOUSE_VIEW_DDL_PARAMETERS.copy()\n",
    "        params[\"schema_map\"] = f\"'{schema_map}'\"\n",
    "        params[\"derived_table_map\"] = derived_table_map\n",
    "        params[\"tables_to_include\"] = tables_to_include\n",
    "\n",
    "\n",
    "        view_ddl = generate_lh_view_ddl(FABRIC_LH_DATABASE, params)\n",
    "      \n",
    "        \n",
    "        logger.info(f\"ðŸ› ï¸ Step 5/7: Execute view DDL on Fabric DW {FABRIC_WH_DATABASE}.\")\n",
    "        execute_ddl_on_warehouse(FABRIC_WH_DATABASE, view_ddl)\n",
    "\n",
    "        if is_not_none_and_empty(SYNAPSE_SERVER) and is_not_none_and_empty(SYNAPSE_DW_DATABASE) :\n",
    " \n",
    "            logger.info(\"ðŸš€ Step 6/7: Fetch view dependencies.\")\n",
    "            views_json = fetch_view_dependencies(\n",
    "                synapse_dw_engine, SYNAPSE_DW_VIEWS,\n",
    "                SYNAPSE_EDL_DATABASE, FABRIC_WH_DATABASE,\n",
    "                SYNAPSE_DW_SCHEMA, FABRIC_WH_SCHEMA\n",
    "            )\n",
    "\n",
    "            logger.info(\"ðŸš€ Step 7/7: Deploy views to warehouse.\")\n",
    "            deploy_views(FABRIC_WH_DATABASE, FABRIC_WH_SCHEMA, views_json)\n",
    "        else:\n",
    "            logger.info(\"ðŸ”¹ Step 6 and 7: Skipped\")\n",
    "\n",
    "        logger.info(\"ðŸŽ‰ Deployment complete.\")\n",
    "    else:\n",
    "        logger.error(\"âŒ Warehouse setup failed.\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "known_lakehouses": []
   },
   "warehouse": {
    "default_warehouse": "09b67508-1825-42b8-b9f4-6946b02e8177",
    "known_warehouses": [
     {
      "id": "09b67508-1825-42b8-b9f4-6946b02e8177",
      "type": "Datawarehouse"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
