{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ea5c26-d6c2-4e71-a58c-217d10d63b4f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Notebook does following \n",
    "\n",
    "- üèóÔ∏è Step 1: Create case-insensitive Data Warehouse.\n",
    "- üîπ Step 2: Connect to Synapse Serverless Database and get table list and schema information.\n",
    "- üîÅ Step 3: Get parent and child tables information from local file.\n",
    "- üõ†Ô∏è Step 4: Connect to Fabric Link lakehouse get table metadata and generate view ddl statements.\n",
    "- üèÉ‚Äç‚ôÇÔ∏è Step 5: Connect to Fabric case in-senstive data warehouse and create views.\n",
    "- üöÄ Step 6: Connect to Synaspe serverless virtual datawarehouse and collect views and dependencies.\n",
    "- üöÄ Step 7: Connect to Fabric datawarehouse and deploy create views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50733e90-a4fc-4424-896b-16399342f769",
   "metadata": {
    "cellStatus": "{\"JJ Yadav\":{\"session_start_time\":\"2025-06-07T13:05:35.2155677Z\",\"execution_start_time\":\"2025-06-07T13:05:39.2399932Z\",\"execution_finish_time\":\"2025-06-07T13:05:40.0626782Z\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-08T12:05:27.9366666Z",
       "execution_start_time": "2025-06-08T12:05:27.6196121Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "54c18d16-8930-4c61-af04-afe29848aa12",
       "queued_time": "2025-06-08T12:05:27.61839Z",
       "session_id": "65f6907a-b648-449a-a793-62399681b712",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 65f6907a-b648-449a-a793-62399681b712, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Configuration Constants ---\n",
    "WORKSPACE_ID        = \"cca1475b-c4fe-417f-844a-f3e8a061e55d\"\n",
    "FABRIC_LH_DATABASE  = \"dataverse_jjunoenvfre\"\n",
    "FABRIC_WH_DATABASE  = \"Sales_DW\"\n",
    "\n",
    "#Optional parameters to connect to your Synapse Serveress to get table and schema info\n",
    "DRIVER              = \"{ODBC Driver 18 for SQL Server}\"\n",
    "SYNAPSE_SERVER      = None # update to example value \"d365analyticsfabricsynapse-ondemand.sql.azuresynapse.net\"  \n",
    "SYNAPSE_EDL_DATABASE= None #update to example value \"analytics.sandbox.operations.dynamics.com\" \n",
    "SYNAPSE_EDL_SCHEMA  = None  #\"dbo\" \n",
    "SYNAPSE_EDL_CONST_COLUMN_NAME = None #=\"_SysRowId\" \n",
    "\n",
    "\n",
    "#Optional parameters to connect to your Synapse Serveress Datawarehouse to get views and dependencies\n",
    "SYNAPSE_DW_DATABASE = None # example value \"Dynamics365_DW\"\n",
    "SYNAPSE_DW_SCHEMA   = None # example value \"dbo\"\n",
    "SYNAPSE_DW_VIEWS    = None # example value \"CustomerDim,SalesFact,SupplierDim\"\n",
    "FABRIC_WH_SCHEMA    = None # example value \"edw\"\n",
    "\n",
    "#Fixed parameter - No need to change\n",
    "GITHUB_RAW_BASE_URL = \"https://raw.githubusercontent.com/microsoft/Dynamics-365-FastTrack-Implementation-Assets/refs/heads/master/Analytics/DataverseLink/FabricLink_SQLAnalyticsEndpoint/DVFabricLinkUtil/\"\n",
    "\n",
    "DERIVED_TABLE_MAP_PATH = \"./builtin/resources/derived_table_map.json\"\n",
    "LH_DDL_TEMPLATE_PATH = \"./builtin/resources/get_lh_ddl_as_view.sql\"\n",
    "VIEW_DEPENDENCY_TEMPLATE_PATH = \"./builtin/resources/get_view_dependency.sql\"\n",
    "REQUIRED_FILES = [\n",
    "    (\"derived_table_map.json\", DERIVED_TABLE_MAP_PATH),\n",
    "    (\"get_lh_ddl_as_view.sql\", LH_DDL_TEMPLATE_PATH),\n",
    "    (\"get_view_dependency.sql\", VIEW_DEPENDENCY_TEMPLATE_PATH)]\n",
    "\n",
    "WAREHOUSE_VIEW_DDL_PARAMETERS = {\n",
    "            \"source_schema\": \"dbo\",\n",
    "            \"target_schema\": \"dbo\",\n",
    "            \"only_fno_tables\": 1,\n",
    "            \"tables_to_include\":\"*\",\n",
    "            \"tables_to_exclude\": \"*\",\n",
    "            \"filter_deleted_rows\": 1,\n",
    "            \"join_derived_tables\": 1,\n",
    "            \"change_collation\": 1,\n",
    "            \"translate_enums\": 1,\n",
    "            \"schema_map\": '[]',\n",
    "            \"derived_table_map\": '[]'\n",
    "        }\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d3a9a-f5c8-4078-a36f-b540e44e6bfd",
   "metadata": {
    "cellStatus": "{\"JJ Yadav\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-07T13:05:40.0644055Z\",\"execution_finish_time\":\"2025-06-07T13:06:28.2937375Z\",\"normalized_state\":\"finished\"}}",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import struct\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from IPython.display import display, JSON\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from requests.exceptions import HTTPError\n",
    "import sys\n",
    "import os\n",
    "\n",
    "synapse_edl_engine = None\n",
    "synapse_dw_engine = None\n",
    "\n",
    "def download_file_if_not_exists(url, local_path):\n",
    "    \"\"\"Download a file from GitHub if it doesn't exist locally.\"\"\"\n",
    "    if not notebookutils.fs.exists(local_path):\n",
    "        notebookutils.fs.mkdirs(os.path.dirname(local_path))\n",
    "        logger.info(f\"‚¨áÔ∏è Downloading {local_path} ...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Fail if not 200 OK\n",
    "        notebookutils.fs.put(local_path, response.content.decode('utf-8'))  # <-- decode bytes to string\n",
    "    else:\n",
    "        logger.info(f\"üìÑ File already exists locally: {local_path}\")\n",
    "\n",
    "def is_not_none_and_empty(s):\n",
    "    return s is not None and s != ''\n",
    "\n",
    "\n",
    "# --- Database Connections ---\n",
    "def create_synapse_engine(server, database):\n",
    "    connection_string = f\"DRIVER={DRIVER};SERVER={server};DATABASE={database};Encrypt=yes;TrustServerCertificate=no;\"\n",
    "    odbc_conn_str = f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(connection_string)}\"\n",
    "    engine = create_engine(odbc_conn_str)\n",
    "    return engine\n",
    "\n",
    "synapse_edl_engine = create_synapse_engine(SYNAPSE_SERVER, SYNAPSE_EDL_DATABASE)\n",
    "synapse_dw_engine = create_synapse_engine(SYNAPSE_SERVER, SYNAPSE_DW_DATABASE)\n",
    "\n",
    "@event.listens_for(synapse_edl_engine, \"do_connect\")\n",
    "@event.listens_for(synapse_dw_engine, \"do_connect\")\n",
    "def inject_access_token(dialect, conn_rec, cargs, cparams):\n",
    "    token = notebookutils.credentials.getToken(\"https://database.windows.net/\")\n",
    "    token_bytes = token.encode(\"utf-16-le\")\n",
    "    token_struct = struct.pack(f\"<I{len(token_bytes)}s\", len(token_bytes), token_bytes)\n",
    "    cparams[\"attrs_before\"] = {1256: token_struct}\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def load_builtin_file(path):\n",
    "    \"\"\"Load a built-in SQL or JSON file.\"\"\"\n",
    "    return notebookutils.fs.head(path)\n",
    "\n",
    "def connect_to_fabric_artifact(artifact_name, workspace_id):\n",
    "    return notebookutils.data.connect_to_artifact(artifact_name, workspace_id)\n",
    "\n",
    "# --- Warehouse Management ---\n",
    "def warehouse_exists(workspace_id, warehouse_name):\n",
    "    try:\n",
    "        connect_to_fabric_artifact(warehouse_name, workspace_id)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if \"ArtifactNotFoundException\" in str(type(e)):\n",
    "            logger.info(f\"üîç Warehouse not found:{warehouse_name}\")\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def create_case_insensitive_warehouse(workspace_id, warehouse_name, retries=5, delay=10):\n",
    "    if warehouse_exists(workspace_id, warehouse_name):\n",
    "        logger.info(f\"‚úÖ Warehouse '{warehouse_name}' already exists.\")\n",
    "        return True\n",
    "\n",
    "    logger.info(f\"üèóÔ∏è Creating new warehouse:{warehouse_name}\")\n",
    "    token = notebookutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"type\": \"Warehouse\",\n",
    "        \"displayName\": warehouse_name,\n",
    "        \"description\": \"New warehouse with case-insensitive collation\",\n",
    "        \"creationPayload\": {\"defaultCollation\": \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"}\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\",\n",
    "        headers=headers, data=json.dumps(payload)\n",
    "    )\n",
    "    logger.info(f\"üì® Warehouse creation response: {response.text}\", )\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        time.sleep(delay)\n",
    "        if warehouse_exists(workspace_id, warehouse_name):\n",
    "            logger.info(\"‚úÖ Warehouse is ready.\")\n",
    "            return True\n",
    "        logger.info(f\"üîÑ Retry {attempt + 1}/{retries}: Warehouse not yet available.\")\n",
    "\n",
    "    logger.error(\"‚ùå Failed to create warehouse after retries.\")\n",
    "    return False\n",
    "\n",
    "# --- Schema Retrieval ---\n",
    "def fetch_tables_and_schema_map(engine):\n",
    "    \"\"\"Fetch table list and schema mapping.\"\"\"\n",
    "    table_list_query = f\"\"\"\n",
    "        SELECT STRING_AGG(CONVERT(NVARCHAR(MAX), TABLE_NAME), ',') AS tablelist\n",
    "        FROM (\n",
    "            SELECT DISTINCT LOWER(TABLE_NAME) as TABLE_NAME\n",
    "            FROM INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' \n",
    "              AND TABLE_NAME IN (\n",
    "                  SELECT DISTINCT TABLE_NAME \n",
    "                  FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                  WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' \n",
    "                    AND COLUMN_NAME = '{SYNAPSE_EDL_CONST_COLUMN_NAME}'\n",
    "              )\n",
    "        ) AS tbl\n",
    "    \"\"\"\n",
    "    schema_map_query = f\"\"\"\n",
    "        SELECT TABLE_NAME as tablename, COLUMN_NAME as columnname,\n",
    "            DATA_TYPE + \n",
    "            CASE \n",
    "                WHEN CHARACTER_MAXIMUM_LENGTH IS NOT NULL THEN '(' + CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)) + ')'\n",
    "                WHEN CHARACTER_MAXIMUM_LENGTH = -1 THEN '(max)'\n",
    "                WHEN DATA_TYPE = 'decimal' THEN '(' + CAST(NUMERIC_PRECISION AS VARCHAR(10)) + ',' + CAST(NUMERIC_SCALE AS VARCHAR(10)) + ')'\n",
    "                ELSE '' \n",
    "            END AS datatype\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}'\n",
    "          AND TABLE_NAME IN (\n",
    "              SELECT DISTINCT TABLE_NAME \n",
    "              FROM INFORMATION_SCHEMA.COLUMNS\n",
    "              WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' AND COLUMN_NAME = '{SYNAPSE_EDL_CONST_COLUMN_NAME}'\n",
    "          )\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        table_list = conn.execute(text(table_list_query)).scalar()\n",
    "    schema_map_df = pd.read_sql(schema_map_query, engine)\n",
    "    schema_map_json = schema_map_df.to_json(orient=\"records\")\n",
    "\n",
    "    return table_list, schema_map_json\n",
    "\n",
    "# --- DDL and View Creation ---\n",
    "def generate_lh_view_ddl(lakehouse_name, params):\n",
    "    \"\"\"Generate view DDL based on Lakehouse tables.\"\"\"\n",
    "    conn = connect_to_fabric_artifact(lakehouse_name, WORKSPACE_ID)\n",
    "\n",
    "    variables_script = f\"\"\"\n",
    "        DECLARE \n",
    "            @source_database_name VARCHAR(200) = '{lakehouse_name}',\n",
    "            @source_table_schema NVARCHAR(10) = '{params[\"source_schema\"]}',\n",
    "            @target_table_schema NVARCHAR(10) = '{params[\"target_schema\"]}',\n",
    "            @TablesToInclude_FnOOnly INT = {params[\"only_fno_tables\"]},\n",
    "            @TablesToIncluce NVARCHAR(MAX) = '{params[\"tables_to_include\"]}',\n",
    "            @TablesToExcluce NVARCHAR(MAX) = '{params[\"tables_to_exclude\"]}',\n",
    "            @filter_deleted_rows INT = {params[\"filter_deleted_rows\"]},\n",
    "            @join_derived_tables INT = {params[\"join_derived_tables\"]},\n",
    "            @change_column_collation INT = {params[\"change_collation\"]},\n",
    "            @translate_enums INT = {params[\"translate_enums\"]},\n",
    "            @schema_map VARCHAR(MAX) = {params[\"schema_map\"]},\n",
    "            @tableinheritance NVARCHAR(MAX) = LOWER('{params[\"derived_table_map\"]}');\n",
    "    \"\"\"\n",
    "    sql_template = load_builtin_file(LH_DDL_TEMPLATE_PATH)\n",
    "    \n",
    "    sqlquery = f\"{variables_script} {sql_template}\"\n",
    "\n",
    "    logger.debug(f\"Debug: generate_lh_view_ddl query {sqlquery}\")\n",
    "    df = conn.query(f\"{sqlquery}\")\n",
    "    ddl_statement = df.iloc[0, 0]\n",
    "    logger.debug(f\"Debug: generate_lh_view_ddl statement {sqlquery}\")\n",
    "\n",
    "    return ddl_statement\n",
    "\n",
    "def execute_ddl_on_warehouse(warehouse_name, ddl_query):\n",
    "    \"\"\"Execute DDL in warehouse.\"\"\"\n",
    "    conn = connect_to_fabric_artifact(warehouse_name, WORKSPACE_ID)\n",
    "    conn.query(ddl_query)\n",
    "\n",
    "# --- View Dependency Management ---\n",
    "def fetch_view_dependencies(engine, root_entities, old_db, new_db, old_schema, new_schema):\n",
    "    sql_template = load_builtin_file(VIEW_DEPENDENCY_TEMPLATE_PATH)\n",
    "    sql_script = f\"\"\"\n",
    "        SET NOCOUNT ON;\n",
    "        DROP TABLE IF EXISTS #myEntitiestree;\n",
    "        DECLARE @entities NVARCHAR(MAX) = '{root_entities}';\n",
    "        DECLARE @old_schema VARCHAR(10) = '{old_schema}';\n",
    "        DECLARE @new_schema VARCHAR(10) = '{new_schema}';\n",
    "        {sql_template}\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(sql_script))\n",
    "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    df[\"definition\"] = df[\"definition\"].str.replace(old_db, new_db, case=False, regex=False)\n",
    "    return df.to_json(orient=\"records\")\n",
    "\n",
    "def deploy_views(warehouse_name, schema, views_json):\n",
    "    execute_ddl_on_warehouse(warehouse_name, f\"IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = '{schema}') BEGIN EXEC('CREATE SCHEMA {schema}'); END;\")\n",
    "    views = json.loads(views_json)\n",
    "    for view in sorted(views, key=lambda x: x[\"depth\"], reverse=True):\n",
    "        if view.get(\"definition\"):\n",
    "            try:\n",
    "                entityname = view[\"entityName\"]\n",
    "                execute_ddl_on_warehouse(warehouse_name, view[\"definition\"])\n",
    "                logger.info(f\"‚úÖ Deployed view: {entityname}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to deploy view {entityname}: {e}\")\n",
    "\n",
    "for handler in logging.root.handlers[:]:  # Clear existing handlers\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]  # Explicitly use stdout\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()  \n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \n",
    "    logger.info(\"üèóÔ∏è Step 1: Download template files if does not exists.\")\n",
    "    # --- Download if not exists ---\n",
    "    for filename, local_path in REQUIRED_FILES:\n",
    "        file_url = f\"{GITHUB_RAW_BASE_URL}/{filename}\"\n",
    "        download_file_if_not_exists(file_url, local_path)\n",
    "        \n",
    "    logger.info(\"üèóÔ∏è Step 2/7: Ensure case-insensitive warehouse exists.\")\n",
    "    \n",
    "    if create_case_insensitive_warehouse(WORKSPACE_ID, FABRIC_WH_DATABASE):\n",
    "\n",
    "        logger.info(\"üîπ Step 3/7: Fetch tables and schema map from Synapse.\")\n",
    "        tables_to_include = '*'\n",
    "        schema_map = '[]'\n",
    "        if is_not_none_and_empty(SYNAPSE_SERVER) and is_not_none_and_empty(SYNAPSE_EDL_DATABASE) :\n",
    "          \n",
    "            tables_to_include, schema_map = fetch_tables_and_schema_map(synapse_edl_engine)\n",
    "        else:\n",
    "            logger.info(\"üîπ Step 3/7: Skipped\")\n",
    "\n",
    "        logger.info(\"üîÅ Step 4/7: Load derived table map.\")\n",
    "        derived_table_map = load_builtin_file(DERIVED_TABLE_MAP_PATH)\n",
    "\n",
    "        logger.info(\"üõ†Ô∏è Step 5/7: Generate view DDL.\")\n",
    "        \n",
    "        # Create a copy of the parameters and update the dynamic fields\n",
    "        params = WAREHOUSE_VIEW_DDL_PARAMETERS.copy()\n",
    "        params[\"schema_map\"] = f\"'{schema_map}'\"\n",
    "        params[\"derived_table_map\"] = derived_table_map\n",
    "        params[\"tables_to_include\"] = tables_to_include\n",
    "\n",
    "\n",
    "        view_ddl = generate_lh_view_ddl(FABRIC_LH_DATABASE, params)\n",
    "      \n",
    "        \n",
    "        logger.info(f\"üõ†Ô∏è Step 5/7: Execute view DDL on Fabric DW {FABRIC_WH_DATABASE}.\")\n",
    "        execute_ddl_on_warehouse(FABRIC_WH_DATABASE, view_ddl)\n",
    "\n",
    "        if is_not_none_and_empty(SYNAPSE_SERVER) and is_not_none_and_empty(SYNAPSE_DW_DATABASE) :\n",
    " \n",
    "            logger.info(\"üöÄ Step 6/7: Fetch view dependencies.\")\n",
    "            views_json = fetch_view_dependencies(\n",
    "                synapse_dw_engine, SYNAPSE_DW_VIEWS,\n",
    "                SYNAPSE_EDL_DATABASE, FABRIC_WH_DATABASE,\n",
    "                SYNAPSE_DW_SCHEMA, FABRIC_WH_SCHEMA\n",
    "            )\n",
    "\n",
    "            logger.info(\"üöÄ Step 7/7: Deploy views to warehouse.\")\n",
    "            deploy_views(FABRIC_WH_DATABASE, FABRIC_WH_SCHEMA, views_json)\n",
    "        else:\n",
    "            logger.info(\"üîπ Step 6 and 7: Skipped\")\n",
    "\n",
    "        logger.info(\"üéâ Deployment complete.\")\n",
    "    else:\n",
    "        logger.error(\"‚ùå Warehouse setup failed.\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "known_lakehouses": []
   },
   "warehouse": {
    "default_warehouse": "09b67508-1825-42b8-b9f4-6946b02e8177",
    "known_warehouses": [
     {
      "id": "09b67508-1825-42b8-b9f4-6946b02e8177",
      "type": "Datawarehouse"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
